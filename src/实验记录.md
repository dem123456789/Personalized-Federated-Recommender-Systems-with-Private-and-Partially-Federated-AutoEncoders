实验记录:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 1, RMSE ≈ 1(最后收敛速度慢)
1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1(最后收敛速度慢)

----

改动平均方法和scheduler后:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1.025





-------

重构代码Pipeline, debug后

1. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=100, epoch=800, dataset=ML100K, rmse ≈ 0.956
2. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=10, epoch=800, dataset=ML100K, rmse ≈ 0.956

-------

Debug后

1. all federated, frac=1, local_epoch=1, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=1, epoch=47, dataset=ML100K, rmse = 0.9177 (Single Node)
2. all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=15, dataset=ML100K, rmse = 0.9260 (Single Node)
1. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=69, dataset=ML100K, rmse = 0.9447
1. all federated, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=500, dataset=ML100K, rmse = 1.3819
1. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=70, dataset=ML100K, rmse =1.0470
1. Decoder, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=47, dataset=ML100K, rmse = 1.5456

总结：当只federate decoder时，rmse高了快10个点。

------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=67, dataset=ML100K, activation function=Tanh, BN=加了第一层, rmse = 0.9220

2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=110, dataset=ML100K, activation function=Tanh, BN=加了所有层, rmse = 0.9475

3. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=150, dataset=ML100K, activation function=Tanh, BN=加了第一层和encoder, rmse = 0.9536

4. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=129, dataset=ML100K, activation function=Tanh, BN=加了第一层和decoder, rmse = 0.9383

5. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=100, dataset=ML100K, activation function=Tanh, BN=加了decoder, rmse = 0.9511

6. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=405, dataset=ML100K, BN=加了第一层, rmse = 0.9850

7. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=542, dataset=ML100K, BN=加了第一层, rmse = 1.0942

   

8. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=107, dataset=ML100K, activation function=ReLU, BN=加了第一层, rmse = 0.9806

9. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=152, dataset=ML100K, activation function=ReLU, BN=加了中间所有层, rmse = 1.0936

10. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=31, dataset=ML100K, activation function=LeakyReLU, BN=加了第一层, rmse = 0.9865

11. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=122, dataset=ML100K, activation function=LeakyReLU, BN=加了中间所有层, rmse = 1.0897

总结: 换的ReLU, LeakyReLU激活函数没有Tanh好用，Batch Normalization无用

----------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9279
2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有层, rmse = 0.9239



1. （测试all federated一个Node）all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=68, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9304
2. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有, rmse = 1.0293
3. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层和encoder, rmse = 1.0504
4. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=235, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9720
5. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=10, epoch=800, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0030
6. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=86, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0375 (只传decoder, rmse比不加LayerNorm少了0.01)
7. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=655, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, 不传LayerNorm, rmse = 0.9537

总结：LayerNorm在num_nodes为1时表现和不加差不多，在num_nodes为10时，rmse上升, 而在Num_nodes为100时，rmse同样上升（上升的少了点)。 并没有什么明显的规律，但是都增加了很多rmse。而在我给encoder加上一层LayerNorm，但是只average decoder, 他的结果相反的，好了一点。

-----

1/19/2022

Research 今天目标:

1. all federated 100个Node做到大概0.93
   1. 检查一遍代码
      1. fraction 0.1，local epoch为5, all federated收敛太慢了(node=10, node=100)，我不知道是什么的问题。尝试node=10, fraction为1, local epoch为1，all federated。收敛太慢 =》代码问题。降倒是降得下去 =》加Lr / 优化fed.py逻辑。
      1. 10个node, fraction为1, local epoch为1, 不federated. test rmse在1.19左右
      1. node=5, rmse=0.9361, epoch=295, fraction=0.2, local_epoch=5; 
      1. node=5, rmse=0.9443, epoch=295, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated); 
      1. node=100, rmse=0.9426, epoch=730, fraction=0.2, local_epoch=5; 
      1. node=100, rmse=0.9453, epoch=730, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated);
      1. node=100, rmse=nan, epoch=130, fraction=0.2, local_epoch=5, 加第一层layernorm(federated);
   2. 加global



-----

1/22/2022

1. 修改更新ratio为1/sum(participated_nodes)
2. 增加global, 修改fed flow
3. 实验记录:
   1. node=100, fraction改为0.1, local epoch改为5，不加global optimizer, rmse=0.9584, epoch=781
   2. node=100, fraction改为0.1, local epoch改为5，加global optimizer
   3. node=100, fraction改为0.1, local epoch改为10，不加global optimizer, rmse=0.9547, epoch=545
   4. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1,其他全为0或者false，要和不加global optimizer数值一样)， rmse=0.9547, epoch=545
   5. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.1), rmse=0.9563, epoch=545
   6. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.2), rmse=0.9582, epoch=545
   7. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.3), rmse=0.9624, epoch=545
   8. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.5), rmse=0.9696, epoch=564
   9. node=100, fraction改为0.1, local epoch改为15，不加global optimizer, rmse=0.9576, epoch=392
4. 总结:
   1. fraction改为0.1, local epoch调整影响不大，并且rmse上升。增加global optimizer后，momentum越大效果越差。

5. 开始采用local_epoch=15, global_epoch=400, 不用global optimizer
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=0.9615, epoch=293
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=0.9579, epoch=293
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=0.9542, epoch=170
   4. node=100, fraction=0.1, local epoch=15，layernorm全，rmse=0.9615, epoch=293

-----

1. 只平均decoder, 开始采用local_epoch=15, global_epoch=400, 不用global optimizer:
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=1.0498, epoch=81
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=1.0538, epoch=81
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=1.0818, epoch=46
   4. node=100, fraction=0.1, local epoch=15，layernorm只加decoder，rmse=1.0831, epoch=46
   4. node=100, fraction=0.1, local epoch=15，no nomalization，rmse=1.0463, epoch=80

-------

1. 测试average all (1M):
   1. 测试local epoch个数的影响, explicit:
      1. node=100, fraction改为0.1, global_epoch=400, local epoch改为5，不加global optimizer, rmse=0.9584, epoch=781
      2. node=100, fraction改为0.1, global_epoch=400, local epoch改为10，不加global optimizer, rmse=0.9547, epoch=545
      3. node=100, fraction改为0.1, global_epoch=800, local epoch改为5，不加global optimizer, rmse=0.9584, epoch=781
      4. node=100, fraction改为0.1, global_epoch=800, local epoch改为10，不加global optimizer, rmse=0.9547, epoch=545
   2. 测试Normalization的影响, explicit, 固定global_epoch为400或者800:
      1. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10，no normalization, 
      2. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10，加batch normalization, rmse=0.9584, epoch=781
      3. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10,  加layernorm
   3. 测试local epoch个数的影响, implicit:
      1. node=100, fraction改为0.1, global_epoch=400, local epoch改为5，不加global optimizer,
      2. node=100, fraction改为0.1, global_epoch=400, local epoch改为10，不加global optimizer, 
      2. node=100, fraction改为0.1, global_epoch=800, local epoch改为5，不加global optimizer,
      2. node=100, fraction改为0.1, global_epoch=800, local epoch改为10，不加global optimizer, 
   4. 测试Normalization的影响, implicit，固定global_epoch为400或者800
      1. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10，no normalization, 
      2. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10，加batch normalization, 
      3. node=100, fraction改为0.1, global_epoch=400, local epoch改为5/10,  加layernorm
2. 测试只average decoder:
   1. 测试local epoch个数的影响, explicit，固定global_epoch为400或者800:
      1. node=100, fraction改为0.1, global_epoch=400,  local epoch=5，不加global optimizer, 
      2. node=100, fraction改为0.1, global_epoch=400,  local epoch=10，不加global optimizer, 
   2. 测试Normalization的影响, explicit:
      1. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，no normalization
      2. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，batchnorm
      3. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，layernorm
   3. 测试local epoch个数的影响, implicit，固定global_epoch为400或者800:
      1. node=100, fraction改为0.1, global_epoch=400, local epoch=5，
      2. node=100, fraction改为0.1, global_epoch=400, local epoch=10，
   4. 测试Normalization的影响, implicit
      1. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，no normalization
      2. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，batchnorm
      3. node=100, fraction改为0.1, global_epoch=400, local epoch=5/10，layernorm

修改代码适配GPU：

1. 直接make_optimizer, 去掉scheduler (done)
2. 模型数据都存到本地dict中 (done)
3. train返回state_dict()
4. combine的时候模型参数.cpu(), 放到self.new_global_model_parameter_dict中
5. update_local_test_model_dict， 先更新参数，然后创建模型再Load_state_dict

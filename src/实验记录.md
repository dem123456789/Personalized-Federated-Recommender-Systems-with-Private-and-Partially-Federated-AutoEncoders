实验记录:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 1, RMSE ≈ 1(最后收敛速度慢)
1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1(最后收敛速度慢)

----

改动平均方法和scheduler后:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1.025





-------

重构代码Pipeline, debug后

1. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=100, epoch=800, dataset=ML100K, rmse ≈ 0.956
2. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=10, epoch=800, dataset=ML100K, rmse ≈ 0.956

-------

Debug后

1. all federated, frac=1, local_epoch=1, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=1, epoch=47, dataset=ML100K, rmse = 0.9177 (Single Node)
2. all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=15, dataset=ML100K, rmse = 0.9260 (Single Node)
1. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=69, dataset=ML100K, rmse = 0.9447
1. all federated, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=500, dataset=ML100K, rmse = 1.3819
1. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=70, dataset=ML100K, rmse =1.0470
1. Decoder, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=47, dataset=ML100K, rmse = 1.5456

总结：当只federate decoder时，rmse高了快10个点。

------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=67, dataset=ML100K, activation function=Tanh, BN=加了第一层, rmse = 0.9220

2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=110, dataset=ML100K, activation function=Tanh, BN=加了所有层, rmse = 0.9475

3. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=150, dataset=ML100K, activation function=Tanh, BN=加了第一层和encoder, rmse = 0.9536

4. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=129, dataset=ML100K, activation function=Tanh, BN=加了第一层和decoder, rmse = 0.9383

5. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=100, dataset=ML100K, activation function=Tanh, BN=加了decoder, rmse = 0.9511

6. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=405, dataset=ML100K, BN=加了第一层, rmse = 0.9850

7. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=542, dataset=ML100K, BN=加了第一层, rmse = 1.0942

   

8. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=107, dataset=ML100K, activation function=ReLU, BN=加了第一层, rmse = 0.9806

9. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=152, dataset=ML100K, activation function=ReLU, BN=加了中间所有层, rmse = 1.0936

10. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=31, dataset=ML100K, activation function=LeakyReLU, BN=加了第一层, rmse = 0.9865

11. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=122, dataset=ML100K, activation function=LeakyReLU, BN=加了中间所有层, rmse = 1.0897

总结: 换的ReLU, LeakyReLU激活函数没有Tanh好用，Batch Normalization无用

----------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9279
2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有层, rmse = 0.9239



1. （测试all federated一个Node）all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=68, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9304
2. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有, rmse = 1.0293
3. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层和encoder, rmse = 1.0504
4. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=235, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9720
5. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=10, epoch=800, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0030
6. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=86, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0375 (只传decoder, rmse比不加LayerNorm少了0.01)
7. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=655, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, 不传LayerNorm, rmse = 0.9537

总结：LayerNorm在num_nodes为1时表现和不加差不多，在num_nodes为10时，rmse上升, 而在Num_nodes为100时，rmse同样上升（上升的少了点)。 并没有什么明显的规律，但是都增加了很多rmse。而在我给encoder加上一层LayerNorm，但是只average decoder, 他的结果相反的，好了一点。

-----

1/19/2022

Research 今天目标:

1. all federated 100个Node做到大概0.93
   1. 检查一遍代码
      1. fraction 0.1，local epoch为5, all federated收敛太慢了(node=10, node=100)，我不知道是什么的问题。尝试node=10, fraction为1, local epoch为1，all federated。收敛太慢 =》代码问题。降倒是降得下去 =》加Lr / 优化fed.py逻辑。
      1. 10个node, fraction为1, local epoch为1, 不federated. test rmse在1.19左右
      1. node=5, rmse=0.9361, epoch=295, fraction=0.2, local_epoch=5; 
      1. node=5, rmse=0.9443, epoch=295, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated); 
      1. node=100, rmse=0.9426, epoch=730, fraction=0.2, local_epoch=5; 
      1. node=100, rmse=0.9453, epoch=730, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated);
      1. node=100, rmse=nan, epoch=130, fraction=0.2, local_epoch=5, 加第一层layernorm(federated);
   2. 加global



-----

1/22/2022

1. 修改更新ratio为1/sum(participated_nodes)
2. 增加global, 修改fed flow
3. 实验记录:
   1. node=100, fraction改为0.1, local epoch改为5，不加global optimizer, rmse=0.9584, epoch=781
   2. node=100, fraction改为0.1, local epoch改为5，加global optimizer
   3. node=100, fraction改为0.1, local epoch改为10，不加global optimizer, rmse=0.9547, epoch=545
   4. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1,其他全为0或者false，要和不加global optimizer数值一样)， rmse=0.9547, epoch=545
   5. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.1), rmse=0.9563, epoch=545
   6. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.2), rmse=0.9582, epoch=545
   7. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.3), rmse=0.9624, epoch=545
   8. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.5), rmse=0.9696, epoch=564
   9. node=100, fraction改为0.1, local epoch改为15，不加global optimizer, rmse=0.9576, epoch=392
4. 总结:
   1. fraction改为0.1, local epoch调整影响不大，并且rmse上升。增加global optimizer后，momentum越大效果越差。

5. 开始采用local_epoch=15, global_epoch=400, 不用global optimizer
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=0.9615, epoch=293
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=0.9579, epoch=293
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=0.9542, epoch=170
   4. node=100, fraction=0.1, local epoch=15，layernorm全，rmse=0.9615, epoch=293

-----

1. 只平均decoder, 开始采用local_epoch=15, global_epoch=400, 不用global optimizer:
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=1.0498, epoch=81
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=1.0538, epoch=81
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=1.0818, epoch=46
   4. node=100, fraction=0.1, local epoch=15，layernorm只加decoder，rmse=1.0831, epoch=46
   4. node=100, fraction=0.1, local epoch=15，no nomalization，rmse=1.0463, epoch=80

-------

1. 一个Node，测试joint:
   1. **node=1, explicit, dataset=1M, rmse=0.8763, epoch=574**
   2. **node=1, implicit, dataset=1M, MAP=0.8447, epoch=395** 

2. 测试average all (1M, 不加global_optimizer):
   1. 测试local epoch个数的影响, explicit:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5, rmse=0.9051, epoch=401**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10, rmse=0.9136, epoch=373**
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5, rmse=0.8933, epoch=752**
      4. **node=100, fraction=0.1, global_epoch=800, local epoch=10, rmse=0.9092, epoch=637** 
      4. **根据2.1.1与2.1.2, 2.1.3与2.1.4的对比, local_epoch=5 > local_epoch=10**
      2. **根据(2.1.1,2.1.2)与(2.1.3,2.1.4)的对比，global_epoch=800 > global_epoch=400**
   2. 测试Normalization的影响, explicit, 选择global_epoch=800, local_epoch=5:
      1. **node=100, fraction=0.1, global_epoch=800, local epoch=5，no normalization, rmse=0.8933, epoch=752 (同2.1.3)** 
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5，加batch normalization, rmse=0.9584, epoch=781, rmse=0.9541, epoch=41** (test_batch=60)
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm,  rmse=0.8867, epoch=752**
      3. **LayerNorm提升了0.0066的效果**
   3. 测试local epoch个数的影响, implicit:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5, MAP=0.8202, epoch=304**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10, MAP=0.8203, epoch=335**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  MAP=0.8202, epoch=304**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=10, MAP=0.8201, epoch=97**
      2. **根据2.3.1与2.3.2, 2.3.3与2.3.4的对比，local_epoch影响不大，选择local_epoch=5**
      2. **根据(2.3.1, 2.3.2)与(2.3.3, 2.3.4), global_epoch影响不大，选择global_epoch=400**
   4. 测试Normalization的影响, implicit，固定global_epoch=400，local_epoch=5
      1. **node=100, fraction改为0.1, global_epoch=400, local epoch改为5，no normalization, MAP=0.8203, epoch=304（同2.3.1）**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch改为5，加batch normalization, （BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=400, local epoch改为5,  加layernorm, MAP=0.8451，epoch=333**
      3. **layernorm对implicit的情况效果非常好，提升了0.0248，甚至超过了node=1的情况**
   5. 测试1个user1个node, explicit
      1. 验证代码修改正确: **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm,  rmse=0.8675, epoch=787**
      1. 验证代码修改正确: **node=1, explicit, dataset=1M, 加layernorm, rmse=0.8713, epoch=632**
      2. **node=100, fraction改为0.1, global_epoch=800, local epoch改为5，加layernorm, rmse=0.9396, epoch=792**
   6. 测试1个user1个node, implicit
      1. node=100, fraction改为0.1, global_epoch=800, local epoch改为5，加layernorm, rmse=0.8203, epoch=304
   
3. 测试只average decoder:
   1. 测试local epoch个数的影响, explicit，固定global_epoch为800:
      1. **node=100, fraction=0.1, global_epoch=800,  local epoch=5, rmse=0.9849, epoch=187**
      2. **node=100, fraction=0.1, global_epoch=800,  local epoch=10, rmse=0.9883, epoch=121**
      2. **验证了explicit在global_epoch为800的情况下，local_epoch=5效果好**
   2. 测试Normalization的影响, explicit:
      1. **node=100, fraction改为0.1, global_epoch=800, local epoch=5，no normalization, rmse=0.9849, epoch=187 (同3.1.1)**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5，batchnorm（BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5，layernorm, rmse=0.9857, epoch=120**
   3. 测试local epoch个数的影响, implicit，固定global_epoch为400:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5，MAP=0.8263, epoch=341**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10，MAP=0.8267, epoch=392**
      2. **验证了implicit在global_epoch为400的情况下，Local_epoch=5和local_epoch=10区别不大，选用Local_epoch=5**
   4. 测试Normalization的影响, implicit
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5，no normalization, MAP=0.8263, epoch=341 (同3.3.1)**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=5，batchnorm （BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=400, local epoch=5，layernorm,MAP=0.8287, epoch=259**
   5. (暂时不测，内存问题) 测试1个user1个node, explicit
   2. (暂时不测，内存问题) 测试1个user1个node, implicit

4. 测试fine tune:

   1. 测试100个node:

      1. 测试epoch个数的影响, explicit:

         1. **node=100, epoch=5, local epoch=5, batch_size=, lr=0.1, optimizer=SGD, rmse=0.9051, epoch=401**
         2. **node=100, epoch=5, local epoch=10, batch_size=, lr=0.1, optimizer=SGD, rmse=0.9051, epoch=401**
         5. **根据2.1.1与2.1.2, 2.1.3与2.1.4的对比, local_epoch=5 > local_epoch=10**
         6. **根据(2.1.1,2.1.2)与(2.1.3,2.1.4)的对比，global_epoch=800 > global_epoch=400**

      2. 测试lr的影响, explicit:

         1. **node=100, epoch=5, local epoch=5, batch_size=, lr=0.01, optimizer=SGD, rmse=0.9051, epoch=401**

         2. **node=100, epoch=5, local epoch=10, batch_size=, lr=0.1, optimizer=SGD, rmse=0.9051, epoch=401**

      3. 测试batch_size的影响, explicit:

      4. 测试local epoch个数的影响, implicit:

         1. **node=100, fraction=0.1, global_epoch=400, local epoch=5, rmse=0.9051, epoch=401**
         2. *node=100, fraction=0.1, global_epoch=400, local epoch=10, rmse=0.9136, epoch=373
         3. **node=100, fraction=0.1, global_epoch=800, local epoch=5, rmse=0.8933, epoch=752**
         4. *node=100, fraction=0.1, global_epoch=800, local epoch=10, rmse=0.9092, epoch=637** 
         5. **根据2.1.1与2.1.2, 2.1.3与2.1.4的对比, local_epoch=5 > local_epoch=10**
         6. *根据(2.1.1,2.1.2)与(2.1.3,2.1.4)的对比，global_epoch=800 > global_epoch=400**

      5. 测试lr的影响, implicit:

      6. 测试batch_size的影响, implicit:

   2. 测试1 user/node

5. 总结:
   1. explicit使用global_epoch=800, local_epoch=5
   2. implicit使用global_epoch=400, local_epoch=5
   3. layernorm在all federated的时候提升较大，只average decoder效果较小或是没有
   4. 只average decoder时，implicit未受较大影响
   5. 只average decoder时，explicit rmse高了将近10个点，并且在epoch较小时就学不到东西了 =》distribution shift

6. 修改:
   1. (Done) 进行1.3和1.4时收敛太慢 => 修改batch_size为10
   2. (Done) 训练太慢 => 整5个colab
   3. 内存不够存储每个user一个模型，如果是all federated可以优化到模型在train时创建, 然后加载global_model的参数然后跑。而如果是只average decoder，该如何处理（需要保存各个encoder的记录), 1M量级都有6000个user, 20M可能就是8w个。（暂时放着）
      1. 去掉local_test_dict
      2. 存本地再读
   4. (Done, BN not working) 处理BN代码 (load_state_dict中strict=False, 不平均running_mean, running_var)
   5. (Done) 优化fed.py流程 （减少内存占用, 修改scheduler为一个epoch更新一次，优化Update_global_model_parameter流程）
   5. 不average layernorm

----

Paper讨论部分:

1. **Layernorm, batchnorm， group norm 区别**

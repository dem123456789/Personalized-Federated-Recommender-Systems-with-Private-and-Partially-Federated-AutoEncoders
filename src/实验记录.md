实验记录:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 1, RMSE ≈ 1(最后收敛速度慢)
1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1(最后收敛速度慢)

----

改动平均方法和scheduler后:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1.025





-------

重构代码Pipeline, debug后

1. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=100, epoch=800, dataset=ML100K, rmse ≈ 0.956
2. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=10, epoch=800, dataset=ML100K, rmse ≈ 0.956

-------

Debug后

1. all federated, frac=1, local_epoch=1, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=1, epoch=47, dataset=ML100K, rmse = 0.9177 (Single Node)
2. all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=15, dataset=ML100K, rmse = 0.9260 (Single Node)
1. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=69, dataset=ML100K, rmse = 0.9447
1. all federated, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=500, dataset=ML100K, rmse = 1.3819
1. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=70, dataset=ML100K, rmse =1.0470
1. Decoder, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=47, dataset=ML100K, rmse = 1.5456

总结：当只federate decoder时，rmse高了快10个点。

------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=67, dataset=ML100K, activation function=Tanh, BN=加了第一层, rmse = 0.9220

2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=110, dataset=ML100K, activation function=Tanh, BN=加了所有层, rmse = 0.9475

3. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=150, dataset=ML100K, activation function=Tanh, BN=加了第一层和encoder, rmse = 0.9536

4. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=129, dataset=ML100K, activation function=Tanh, BN=加了第一层和decoder, rmse = 0.9383

5. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=100, dataset=ML100K, activation function=Tanh, BN=加了decoder, rmse = 0.9511

6. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=405, dataset=ML100K, BN=加了第一层, rmse = 0.9850

7. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=542, dataset=ML100K, BN=加了第一层, rmse = 1.0942

   

8. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=107, dataset=ML100K, activation function=ReLU, BN=加了第一层, rmse = 0.9806

9. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=152, dataset=ML100K, activation function=ReLU, BN=加了中间所有层, rmse = 1.0936

10. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=31, dataset=ML100K, activation function=LeakyReLU, BN=加了第一层, rmse = 0.9865

11. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=122, dataset=ML100K, activation function=LeakyReLU, BN=加了中间所有层, rmse = 1.0897

总结: 换的ReLU, LeakyReLU激活函数没有Tanh好用，Batch Normalization无用

----------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9279
2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有层, rmse = 0.9239



1. （测试all federated一个Node）all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=68, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9304
2. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有, rmse = 1.0293
3. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层和encoder, rmse = 1.0504
4. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=235, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9720
5. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=10, epoch=800, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0030
6. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=86, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0375 (只传decoder, rmse比不加LayerNorm少了0.01)
7. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=655, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, 不传LayerNorm, rmse = 0.9537

总结：LayerNorm在num_nodes为1时表现和不加差不多，在num_nodes为10时，rmse上升, 而在Num_nodes为100时，rmse同样上升（上升的少了点)。 并没有什么明显的规律，但是都增加了很多rmse。而在我给encoder加上一层LayerNorm，但是只average decoder, 他的结果相反的，好了一点。

-----

1/19/2022

Research 今天目标:

1. all federated 100个Node做到大概0.93
   1. 检查一遍代码
      1. fraction 0.1，local epoch为5, all federated收敛太慢了(node=10, node=100)，我不知道是什么的问题。尝试node=10, fraction为1, local epoch为1，all federated。收敛太慢 =》代码问题。降倒是降得下去 =》加Lr / 优化fed.py逻辑。
      1. 10个node, fraction为1, local epoch为1, 不federated. test rmse在1.19左右
      1. node=5, rmse=0.9361, epoch=295, fraction=0.2, local_epoch=5; 
      1. node=5, rmse=0.9443, epoch=295, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated); 
      1. node=100, rmse=0.9426, epoch=730, fraction=0.2, local_epoch=5; 
      1. node=100, rmse=0.9453, epoch=730, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated);
      1. node=100, rmse=nan, epoch=130, fraction=0.2, local_epoch=5, 加第一层layernorm(federated);
   2. 加global




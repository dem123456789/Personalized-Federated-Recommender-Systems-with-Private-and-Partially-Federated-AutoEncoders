

​	



实验记录:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 1, RMSE ≈ 1(最后收敛速度慢)
1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1(最后收敛速度慢)

----

改动平均方法和scheduler后:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1.025





-------

重构代码Pipeline, debug后

1. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=100, epoch=800, dataset=ML100K, rmse ≈ 0.956
2. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=10, epoch=800, dataset=ML100K, rmse ≈ 0.956

-------

Debug后

1. all federated, frac=1, local_epoch=1, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=1, epoch=47, dataset=ML100K, rmse = 0.9177 (Single Node)
2. all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=15, dataset=ML100K, rmse = 0.9260 (Single Node)
1. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=69, dataset=ML100K, rmse = 0.9447
1. all federated, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=500, dataset=ML100K, rmse = 1.3819
1. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=70, dataset=ML100K, rmse =1.0470
1. Decoder, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=47, dataset=ML100K, rmse = 1.5456

总结：当只federate decoder时，rmse高了快10个点。

------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=67, dataset=ML100K, activation function=Tanh, BN=加了第一层, rmse = 0.9220

2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=110, dataset=ML100K, activation function=Tanh, BN=加了所有层, rmse = 0.9475

3. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=150, dataset=ML100K, activation function=Tanh, BN=加了第一层和encoder, rmse = 0.9536

4. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=129, dataset=ML100K, activation function=Tanh, BN=加了第一层和decoder, rmse = 0.9383

5. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=100, dataset=ML100K, activation function=Tanh, BN=加了decoder, rmse = 0.9511

6. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=405, dataset=ML100K, BN=加了第一层, rmse = 0.9850

7. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=542, dataset=ML100K, BN=加了第一层, rmse = 1.0942

   

8. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=107, dataset=ML100K, activation function=ReLU, BN=加了第一层, rmse = 0.9806

9. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=152, dataset=ML100K, activation function=ReLU, BN=加了中间所有层, rmse = 1.0936

10. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=31, dataset=ML100K, activation function=LeakyReLU, BN=加了第一层, rmse = 0.9865

11. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=122, dataset=ML100K, activation function=LeakyReLU, BN=加了中间所有层, rmse = 1.0897

总结: 换的ReLU, LeakyReLU激活函数没有Tanh好用，Batch Normalization无用

----------

1. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9279
2. Joint, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有层, rmse = 0.9239



1. （测试all federated一个Node）all federated, frac=1, local_epoch=1, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=68, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9304
2. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了所有, rmse = 1.0293
3. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=66, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层和encoder, rmse = 1.0504
4. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=235, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 0.9720
5. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=10, epoch=800, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0030
6. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=86, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, rmse = 1.0375 (只传decoder, rmse比不加LayerNorm少了0.01)
7. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=655, dataset=ML100K, activation function=Tanh, LayerNorm=加了第一层, 不传LayerNorm, rmse = 0.9537

总结：LayerNorm在num_nodes为1时表现和不加差不多，在num_nodes为10时，rmse上升, 而在Num_nodes为100时，rmse同样上升（上升的少了点)。 并没有什么明显的规律，但是都增加了很多rmse。而在我给encoder加上一层LayerNorm，但是只average decoder, 他的结果相反的，好了一点。

-----

1/19/2022

Research 今天目标:

1. all federated 100个Node做到大概0.93
   1. 检查一遍代码
      1. fraction 0.1，local epoch为5, all federated收敛太慢了(node=10, node=100)，我不知道是什么的问题。尝试node=10, fraction为1, local epoch为1，all federated。收敛太慢 =》代码问题。降倒是降得下去 =》加Lr / 优化fed.py逻辑。
      1. 10个node, fraction为1, local epoch为1, 不federated. test rmse在1.19左右
      1. node=5, rmse=0.9361, epoch=295, fraction=0.2, local_epoch=5; 
      1. node=5, rmse=0.9443, epoch=295, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated); 
      1. node=100, rmse=0.9426, epoch=730, fraction=0.2, local_epoch=5; 
      1. node=100, rmse=0.9453, epoch=730, fraction=0.2, local_epoch=5, 加第一层layernorm(不federated);
      1. node=100, rmse=nan, epoch=130, fraction=0.2, local_epoch=5, 加第一层layernorm(federated);
   2. 加global



-----

1/22/2022

1. 修改更新ratio为1/sum(participated_nodes)
2. 增加global, 修改fed flow
3. 实验记录:
   1. node=100, fraction改为0.1, local epoch改为5，不加global optimizer, rmse=0.9584, epoch=781
   2. node=100, fraction改为0.1, local epoch改为5，加global optimizer
   3. node=100, fraction改为0.1, local epoch改为10，不加global optimizer, rmse=0.9547, epoch=545
   4. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1,其他全为0或者false，要和不加global optimizer数值一样)， rmse=0.9547, epoch=545
   5. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.1), rmse=0.9563, epoch=545
   6. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.2), rmse=0.9582, epoch=545
   7. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.3), rmse=0.9624, epoch=545
   8. node=100, fraction改为0.1, local epoch改为10，加global optimizer(lr=1, momentum=0.5), rmse=0.9696, epoch=564
   9. node=100, fraction改为0.1, local epoch改为15，不加global optimizer, rmse=0.9576, epoch=392
4. 总结:
   1. fraction改为0.1, local epoch调整影响不大，并且rmse上升。增加global optimizer后，momentum越大效果越差。

5. 开始采用local_epoch=15, global_epoch=400, 不用global optimizer
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=0.9615, epoch=293
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=0.9579, epoch=293
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=0.9542, epoch=170
   4. node=100, fraction=0.1, local epoch=15，layernorm全，rmse=0.9615, epoch=293

-----

1. 只平均decoder, 开始采用local_epoch=15, global_epoch=400, 不用global optimizer:
   1. node=100, fraction=0.1, local epoch=15，layernorm加第一层, rmse=1.0498, epoch=81
   2. node=100, fraction=0.1, local epoch=15，layernorm加第一层和encoder一层，rmse=1.0538, epoch=81
   3. node=100, fraction=0.1, local epoch=15，layernorm加encoder一层和decoder一层，rmse=1.0818, epoch=46
   4. node=100, fraction=0.1, local epoch=15，layernorm只加decoder，rmse=1.0831, epoch=46
   4. node=100, fraction=0.1, local epoch=15，no nomalization，rmse=1.0463, epoch=80

-------

1. 一个Node，测试joint:
   1. **node=1, explicit, dataset=1M, rmse=0.8763, epoch=574**
   2. **node=1, implicit, dataset=1M, MAP=0.8447, epoch=395** 

2. 测试average all (1M, 不加global_optimizer):
   1. 测试local epoch个数的影响, explicit:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5, rmse=0.9051, epoch=401**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10, rmse=0.9136, epoch=373**
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5, rmse=0.8933, epoch=752**
      4. **node=100, fraction=0.1, global_epoch=800, local epoch=10, rmse=0.9092, epoch=637** 
      4. **根据2.1.1与2.1.2, 2.1.3与2.1.4的对比, local_epoch=5 > local_epoch=10**
      2. **根据(2.1.1,2.1.2)与(2.1.3,2.1.4)的对比，global_epoch=800 > global_epoch=400**
   2. 测试Normalization的影响, explicit, 选择global_epoch=800, local_epoch=5:
      1. **node=100, fraction=0.1, global_epoch=800, local epoch=5，no normalization, rmse=0.8933, epoch=752 (同2.1.3)** 
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5，加batch normalization, rmse=0.9584, epoch=781, rmse=0.9541, epoch=41** (test_batch=60)
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm,  rmse=0.8867, epoch=752**
      3. **LayerNorm提升了0.0066的效果**
   3. 测试local epoch个数的影响, implicit:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5, MAP=0.8202, epoch=304**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10, MAP=0.8203, epoch=335**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  MAP=0.8202, epoch=304**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=10, MAP=0.8201, epoch=97**
      2. **根据2.3.1与2.3.2, 2.3.3与2.3.4的对比，local_epoch影响不大，选择local_epoch=5**
      2. **根据(2.3.1, 2.3.2)与(2.3.3, 2.3.4), global_epoch影响不大，选择global_epoch=400**
   4. 测试Normalization的影响, implicit，固定global_epoch=400，local_epoch=5
      1. **node=100, fraction改为0.1, global_epoch=400, local epoch改为5，no normalization, MAP=0.8203, epoch=304（同2.3.1）**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch改为5，加batch normalization, （BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=400, local epoch改为5,  加layernorm, MAP=0.8451，epoch=333**
      3. **layernorm对implicit的情况效果非常好，提升了0.0248，甚至超过了node=1的情况**
   5. 测试1个user1个node, explicit
      1. 验证代码修改正确: 
         1. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=False, global_scheduler_lr=True,  rmse=0.8675 / 0.8652, epoch=787 / 794**
         2. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=False,  rmse=0.9096, epoch=782**
         3. **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  rmse=0.8637, epoch=796**
         4. **node=1, explicit, dataset=1M, 加layernorm, rmse=0.8713, epoch=632**
      2. **node=max, fraction改为0.1, global_epoch=800, local epoch改为5，加layernorm, global_scheduler_lr=False, rmse=0.9396, epoch=792**
      3. (Colab断了)**node=max, fraction改为0.1, global_epoch=800, local epoch改为5，加layernorm, global_scheduler_lr=True, rmse=0.9640, epoch=712**
      4. *node=max, fraction改为0.1, global_epoch=800, local epoch改为1，加layernorm, global_scheduler_lr=True, rmse=0.9640, epoch=712*
   6. 测试1个user1个node, implicit
      1. (Colab断了)**node=max, fraction改为0.1, global_epoch=800, local epoch改为5，加layernorm, global_scheduler_lr=True, MAP=0.8546, epoch=602**
   7. 1user/node总结:
      1. 运行速度非常慢，大概跑完要10个小时左右(P-100)
      2. explicit的情况下, 暂时不使用global_scheduler_lr下降的稍微快一点，但是都还有下降空间 => 增加epoch或者调整参数
      3. implicit的情况下, 效果比100node（MAP=0.8472）要好很多，能到0.8546的MAP，并且收敛很快，到0.8MAP只需要15个epoch
   
3. 测试只average decoder:
   1. 测试local epoch个数的影响, explicit，固定global_epoch为800:
      1. **node=100, fraction=0.1, global_epoch=800,  local epoch=5, rmse=0.9849, epoch=187**
      2. **node=100, fraction=0.1, global_epoch=800,  local epoch=10, rmse=0.9883, epoch=121**
      2. **验证了explicit在global_epoch为800的情况下，local_epoch=5效果好**
   2. 测试Normalization的影响, explicit:
      1. **node=100, fraction改为0.1, global_epoch=800, local epoch=5，no normalization, rmse=0.9849, epoch=187 (同3.1.1)**
      2. **node=100, fraction=0.1, global_epoch=800, local epoch=5，batchnorm（BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=800, local epoch=5，layernorm, rmse=0.9857, epoch=120**
   3. 测试local epoch个数的影响, implicit，固定global_epoch为400:
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5，MAP=0.8263, epoch=341**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=10，MAP=0.8267, epoch=392**
      2. **验证了implicit在global_epoch为400的情况下，Local_epoch=5和local_epoch=10区别不大，选用Local_epoch=5**
   4. 测试Normalization的影响, implicit
      1. **node=100, fraction=0.1, global_epoch=400, local epoch=5，no normalization, MAP=0.8263, epoch=341 (同3.3.1)**
      2. **node=100, fraction=0.1, global_epoch=400, local epoch=5，batchnorm （BN无效果，直接放弃）**
      3. **node=100, fraction=0.1, global_epoch=400, local epoch=5，layernorm,MAP=0.8287, epoch=259**
   5. (暂时不测，内存问题) 测试1个user1个node, explicit
   2. (暂时不测，内存问题) 测试1个user1个node, implicit

4. 测试fine tune:

   1. 测试100个node:

      1. Base Line:

         1. (Explicit) **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  rmse=0.8636, epoch=796**
         2. (Implicit) **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  MAP=0.8472, epoch=666**

      2. 测试epoch个数的影响, explicit:

         1. **node=100, epoch=5, batch_size=10, lr=0.1, optimizer=SGD, rmse=0.8636, epoch=796** (0.8636 => 0.8660 => 0.8696 => 0.8732 => 0.8765 => 0.8795)
         2. **node=100, epoch=10, batch_size=10, lr=0.1, optimizer=SGD, rmse=0.8636, epoch=796** (0.8636 => 0.8660 => 0.8701 => 0.8743, etc)
         3. Fine tune的rmse随着epoch增多一直上升，选择epoch=5

      3. 测试lr的影响, explicit:

         1. **node=100, epoch=5, batch_size=10, lr=0.1, optimizer=SGD, rmse=0.8636, epoch=796** (0.8636 => 0.8660 => 0.8696 => 0.8732 => 0.8765 => 0.8795)
         2. **node=100, epoch=5, batch_size=10, lr=0.01, optimizer=SGD, rmse=0.8636, epoch=796** (0.8636 => 0.8641 => 0.8646 => 0.8650 => 0.8654 => 0.8658)
         2. lr不同的情况下rmse一直在增加，lr小的时候增加的慢，选用lr=0.01

      4. 测试batch_size的影响, explicit:

         1. **node=100, epoch=5, batch_size=5, lr=0.01, optimizer=SGD, rmse=0.8636, epoch=796** (0.8636 => 0.8648 => 0.8660 => 0.8671 => 0.8681 => 0.8690)

         2. **node=100, epoch=5, batch_size=10, lr=0.01, optimizer=SGD, rmse=0.8636, epoch=796**

            (0.8636 => 0.8641 => 0.8646 => 0.8650 => 0.8654 => 0.8658)

         3. Batch size无明显影响，batch_size=5上升的稍比batch_size=10快

      5. 测试epoch个数的影响, implicit:

         1. **node=100, epoch=5, batch_size=10, lr=0.1, optimizer=SGD, MAP=0.8468, epoch=666** (0.8472 => 0.8468 => 0.8463 => 0.8458 => 0.8454 => 0.8450)
         2. **node=100, epoch=10, batch_size=10, lr=0.1, optimizer=SGD, MAP=0.8469, epoch=666**
         2. Fine tune的MAP随着epoch增多一直下降，选择epoch=5

      6. 测试lr的影响, implicit:

         1. **node=100, epoch=5, batch_size=10, lr=0.1, optimizer=SGD, MAP=0.8468, epoch=666 **(0.8472 => 0.8468 => 0.8463 => 0.8458 => 0.8454 => 0.8450)
         2. **node=100, epoch=5, batch_size=10, lr=0.01, optimizer=SGD, MAP=0.8472, epoch=666** (0.8472 => 0.8472 => 0.8469 => 0.8468 => 0.8467 => 0.8466)
         2. lr不同的情况下也是一直在降，lr小的时候降低的慢，选用lr=0.01

      7. 测试batch_size的影响, implicit:
         1. **node=100, epoch=5, batch_size=5, lr=0.01, optimizer=SGD, MAP=0.8471, epoch=666**（0.8472 => 0.8469 => 0.8467 => 0.8466 => 0.8464）

         2. **node=100, epoch=5, batch_size=10, lr=0.01, optimizer=SGD, MAP=0.8472, epoch=666**

            (0.8472 => 0.8472 => 0.8469 => 0.8468 => 0.8467 => 0.8466)

         2. Batch size无明显影响，batch_size=5下降的稍比batch_size=10快

      8. 总结: 

         1. 对于100个Node的fine tune，在explicit与implicit模式下, 都在让结果变得更差。

   2. （暂时放着）测试1 user/node

5. 总结:
   1. explicit使用global_epoch=800, local_epoch=5
   2. implicit使用global_epoch=400, local_epoch=5
   3. layernorm在all federated的时候提升较大，只average decoder效果较小或是没有
   4. 只average decoder时，implicit未受较大影响
   5. 只average decoder时，explicit rmse高了将近10个点，并且在epoch较小时就学不到东西了 =》distribution shift

6. 修改:
   1. (Done) 进行1.3和1.4时收敛太慢 => 修改batch_size为10
   
   2. (Done) 训练太慢 => 整5个colab
   
   3. 内存不够存储每个user一个模型，如果是all federated可以优化到模型在train时创建, 然后加载global_model的参数然后跑。而如果是只average decoder，该如何处理（需要保存各个encoder的记录), 1M量级都有6000个user, 20M可能就是8w个。（暂时放着）
      1. 去掉local_test_dict
      2. 存本地再读
      
   4. (Done, BN not working) 处理BN代码 (load_state_dict中strict=False, 不平均running_mean, running_var)
   
   5. (Done) 优化fed.py流程 （减少内存占用, 修改scheduler为一个epoch更新一次，优化Update_global_model_parameter流程）
   
   6. 不average layernorm
   
   7. 实验补充 2/9/2022:
      1. **1 user/node local_epoch=1的情况**，**使用global_optimizer_lr与否的figure**
      2. **Implement FedSGD**
      3. **Fix data split, 验证fine tune**
      4. 总结:
         1. 1 user/node local_epoch=1, 下降非常慢
         2. Figure
         3. Implement FedSGD, 卡在0.98 (用1个Model做fedavg不行，还是要复制model)
         4. Fix data split, 验证fine tune
            1. 存data_split
            2. 在seed固定的情况下，每次分的一样
      
   8. 实验补充 2/10/2022:
      1. Fedsgd: 1 batch train 5/10/20 times => 0.86
         1. Local_epoch=5, rmse=0.8717, epoch=773
         2. Local_epoch=10, rmse=0.8505, epoch=785
         3. Local_epoch=15, rmse=0.8500, epoch=785
         4. Local_epoch=20, rmse=0.8506, epoch=785
      2. Fedavg: Partially average during local epoch / 权重
      3. FineTune: 固定前面的网络, initialize (xavier)最后一层，训练最后一层, 固定decoder, 固定encoder
      4. Change scheduler for 不寻常曲线
      5. Fedsgd, Fedavg, Fedpartial对比 (600 nodes, local_epoch=5):
         1. Fedsgd:
            1. 每一个Local_epoch, 平均全局模型
         2. Fedavg:
            1. 跑完5个Local_epoch, 平均全局模型
         3. Fedpartial (假设partial ratio=0.1):
            1. 540个nodes跑完Local_epoch=5的fedavg, 60个nodes跑完5次local_epoch=1的fedavg（或者说fedsgd）后平均全局模型  
      6. 总结:
         1. upper bound checks
         2. Partial Average (global_epoch=800, local_epoch=5):
            1. 每个Local epoch随机抽取0.1 (不能保存local optimizer state dict):
               1. rmse=0.9808, epoch=523 (暂停)
            2. 每个Local epoch随机抽取0.2 (不能保存local optimizer state dict):
               1. rmse=0.99, epoch=670 (暂停)
            3. 运行Local epoch前固定0.1要平均的clients (不平均的client用自己的local optimizer state dict)
               1. rmse=0.9760, epoch=777
            4. 运行Local epoch前固定0.3要平均的clients (不平均的client用自己的local optimizer state dict)
               1. rmse=0.9770, epoch=780
            5. 影响因素: fedsgd与fedavg local_epoch数量影响，占比影响，占比权重影响
            6. 运行Local epoch前固定0.1要平均的clients, 使用更号函数重新定义权重
               1. rmse=0.9761, epoch=724
            7. 固定0.1, 0.3, 减少fedsgd local_epoch， 平均占比，效果变差
            8. 固定0.1, 0.3, 增加fedsgd local_epoch, 增加占比，效果变差
            8. **以上是因为每次partial average model更新完, 本地的新optimizer只跑了一次, partial average optimizer为空的情况下一次无效果，要>=2，local_computing开销占比小**
            10. 影响因素: 占比影响, fedpartial本地partial epoch影响，占比权重影响, 每次Local_epoch shuffle与否的影响, 截取在global_epoch=100比较
                1. Base case:
                   1. **partial average=0, rmse=1.0688, epoch=100 (rmse=0.9641, epoch=643)**
                2. 固定抽取
                   1. 测试占比影响:
                      1. **partial average=0.1, partial_epoch=2, 占比=avg, rmse=1.0153, epoch=100 (rmse=385, epoch=0.9619，截取global_epoch=400)**
                      2. **partial average=0.2, partial_epoch=2, 占比=avg, rmse=1.0113, epoch=100  (rmse=415, epoch=0.9610, 截取global_epoch=400)**
                      3. **Partial average=0.3, partial_epoch=2, 占比=avg, rmse=1.0082, epoch=100**
                      4. **Partial average=0.5, partial_epoch=2, 占比=avg, rmse=1.0097, epoch=100**
                      5. 相比base case, 选取一小部分做partial的'fedavg'可以让他收敛更快，可能不会增加communication cost, 因为收敛更快
                   2. 测试fedpartial本地epoch影响:
                      1. **partial average=0.1, partial_epoch=2, 占比=avg, rmse=1.0153, epoch=100** 
                      2. **partial average=0.1, partial_epoch=5, 占比=avg, rmse=0.9728, epoch=100**
                      3. **partial average=0.1, partial_epoch=10, 占比=avg, rmse=0.9834, epoch=100 (rmse=0.9350, epoch=610)**
                      4. Partial_epoch为5时，表现最好
                   3. 测试占比权重:
                      1. **partial average=0.1, partial_epoch=5, 占比=avg, rmse=0.9728, epoch=100**
                      2. **partial average=0.1, partial_epoch=5, 占比=sqrt, rmse=0.9774, epoch=100**
                      3. 使用sqrt函数一开始收敛速度快，而后降低，可以测试其他函数
                3. **随机抽取(没抽到的client始终跑2个partial_epoch, 现实情况无法每次维持原本的0.1client, 可以实验每次0.1client中多少不变(0.09)，多少变(0.01))**
                   1. 测试占比影响:
                      1. **partial average=0.1, partial_epoch=2, 占比=avg, rmse=1.0614, epoch=100 （rmse=0.9670, epoch=600）**
                      2. **partial average=0.2, partial_epoch=2, 占比=avg, rmse=1.2263, epoch=100 (rmse=0.9997, epoch=701)** 
                      3. Partial average=0.3, partial_epoch=2, 占比=avg
                      4. Partial average=0.5, partial_epoch=2, 占比=avg
                      4. 随机抽取client而后平均，无法帮助模型快速收敛，数据分布变了
                   2. 测试fedpartial本地epoch影响:
                      1. **partial average=0.1, partial_epoch=2, 占比=avg, rmse=1.0614, epoch=100** 
                      2. **partial average=0.1, partial_epoch=5, 占比=avg, rmse=1.0640, epoch=100**
                      3. **partial average=0.1, partial_epoch=10, 占比=avg, rmse=1.0630, epoch=100**
                   3. 测试占比权重:
                      1. partial average=0.1, partial_epoch=5, 占比=avg
                      2. partial average=0.1, partial_epoch=5, 占比=sqrt
         3. Fine tune
            1. 初始化decoder最后一层或者decoder，影响很大, 结果剧烈变差, 而后只能收缩到rmse=1.2
            2. 初始化encoder, 结果稍微变差 （0.001至0.01级别, 取决于lr）
            3. fine tune不起作用, 初始化encoder不能带来个性化
            4. decoder剧烈变差，可能是由于网络过浅/模型问题/数据问题
         4. Change scheduler:
            1. 换成multistep后，下降曲线符合预期，但是test rmse并不好 => 0.90-0.91![image-20220214161306433](/Users/qile/Library/Application Support/typora-user-images/image-20220214161306433.png)
      
   9. 实验补充2/20/2022
      1. Fine Tune:
         1. 测试100个node:
            1. Base Line:
   
               1. (Explicit) **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  rmse=0.8621, epoch=796**
               2. (Implicit) **node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  MAP=0.8472, epoch=666**
            2. 测试fine tune all:
               1. 测试lr的影响, explicit:
                  1. **node=100, epoch=30, batch_size=10, lr=0.1, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=0.8884 test_epoch=1**
                  2. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=0.8611, test_epoch=3**
                  3. fine tune时lr要小
               2. 测试optimizer的影响, explicit, 修改Momentum为0，防止下降过快和偏差:
                  1. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=0.8611, test_epoch=3**
                  2. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0, scheduler=cosine, rmse=0.8565, test_epoch=26**
                  3. momentum=0效果好
               3. 测试scheduler的影响, explicit:
                  1. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0, scheduler=cosine, rmse=0.8565, test_epoch=26**
                  2. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0, scheduler=Multistep, rmse=0.8564, test_epoch=30**
                  3. 换scheduler差别不大, 还是使用cosine
               4. 测试batch_size的影响, explicit:
                  1. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0, scheduler=cosine, rmse=0.8565, test_epoch=26**
                  2. **node=100, epoch=30, batch_size=5, lr=0.01, optimizer=SGD, momentum=0, scheduler=cosine, rmse=0.8536, test_epoch=21**
                  3. Batch_size小的时候下降更快
            3. 测试fine tune last layer:
               1. 测试optimizer momentum的影响, explicit, 修改Momentum为0，防止下降过快和偏差:
                  1. **node=100, epoch=30, batch_size=10, lr=0.01, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=1.44, test_epoch=30**
                  2. **node=100, epoch=30, batch_size=5, lr=0.1, optimizer=SGD, momentum=0, scheduler=cosine, rmse=2.19 test_epoch=30**
                  3. 选择momentum=0.9，下降更快
               2. 测试lr的影响, explicit:
                  1. **node=100, epoch=30, batch_size=5, lr=0.1, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=1.44 test_epoch=30**
                  2. **node=100, epoch=30, batch_size=5, lr=0.3, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=1.4285, test_epoch=30**
                  3. **node=100, epoch=30, batch_size=5, lr=0.5, optimizer=SGD, momentum=0.9, scheduler=cosine, rmse=1.4549, test_epoch=30**
                  4. 调整lr无效，30个epoch已经下降的很慢并且train rmse到了0.58
               3. 测试scheduler的影响, explicit:
                  1. **node=100, epoch=30, batch_size=5, lr=0.1, optimizer=SGD, momentum=0.9, scheduler=Multistep, rmse=1.4424 test_epoch=30**
                  2. **node=100, epoch=30, batch_size=5, lr=0.3, optimizer=SGD, momentum=0.9, scheduler=Multistep, rmse=1.4256, test_epoch=30**
                  3. **node=100, epoch=30, batch_size=5, lr=0.5, optimizer=SGD, momentum=0.9, scheduler=Multistep, rmse=1.4545, test_epoch=30**
                  3. 换scheduler差别不大
      2. Scheduler
         1. optimizer
            1. SGD
               1. if weight decay != 0 (正则化):
                  1. 更新gradient使用weght decay✖️上一次参数
               2. if momentum != 0:
                  1. 更新b使用momentum✖️上一次Momentum + (1-dampening)✖️gradient, dampening一般为0, 初始化为b=graident
                  2. if nesterov:
                     1. 更新gradient使用上一次gradient+momentum✖️b
                  3. else:
                     1. gradient = b
               3. Momentum: 使梯度更新具备一些先知，epoch较小时，预测有偏差并且如果方向一直下降，速度会很快
               4. nesterov: 拿着上一步的速度先走一小步，再看当前的梯度然后再走一步
            2. Adam
               1. 梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑
            3. LBFGS
               1. 拟牛顿法，使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根, 二阶收敛, 牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。
         2. Scheduler
            1. StepLR (有序调整)
               1. 每隔step_size, lr*=gamma
            2. MultiStepLR (有序调整)
               1. index=0, 当epoch in Milestons list(increasing)[index], lr*=gamma, index+=1
            3. ExponentialLR (有序调整)
               1. 每个epoch, lr*=gamma
            4. ConsineAnnealingLR (有序调整)
               1. ![image-20220220183431645](/Users/qile/Library/Application Support/typora-user-images/image-20220220183431645.png)
               2. eta_min: 最低学习率， eta_max，最高学习率，T_max: global epoch数量，T_cur: 当前epoch数(无restart)
               3. 是不是锯齿形取决于Tmax设置
               4. lr 首先缓慢下降，然后加速下降，再次缓慢下降
            5. ReduceLROnPlateau (自适应调整)
               1. 当某指标(loss或accuracy)在最近几个epoch中都没有变化(下降或升高超过给定阈值)时，调整学习率（*=gamma）。
            6. CyclicLR
               1. 锯齿形来回波动
         3. 实验：
            1. 如图
      3. partial average with local optimizer
         1. baseline:
            1. Upperbound:
               1. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=False, average_ratio=1, rmse=0.8627, test_epoch=800** （rmse=0.9172, epoch=200）
               2. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=True, average_ratio=1（rmse=0.9172, epoch=200）**
               3. shuffle与不shuffle在average_ratio=1的时候，epoch=200的时候, rmse都为0.9172, upperbound checks
            2. Lowerbound:
               1. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=False, average_ratio=0, rmse=0.8782, test_epoch=800** （rmse=**0.9395**, epoch=200, 0.9401->0.9398->0.9395(198->200)）(跑train_privacy_federated_all_partial_average.py)
               2. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, rmse与上条相同**  (跑train_privacy_federated_all.py)
               3. 一次都不平均, lower bound checks
            3. Not shuffle
               1. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=False, average_ratio=0.1, rmse=0.8831, test_epoch=795, rmse=0.9405, test_epoch=200** （0.9423 -> 0.9408 ->0.9405）
               2. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=False, average_ratio=0.2, rmse=0.8717, test_epoch=788, rmse=0.9366, test_epoch=200** (0.9378 -> 0.9356 -> 0.9366)
               3. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=False, average_ratio=0.3, rmse=0.8722, test_epoch=785, rmse=0.9327, test_epoch=200** (0.9339 -> 0.9318 -> 0.9327)
            4. Shuffle:
               1. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=True, average_ratio=0.1, rmse=0.8746, test_epoch=788, rmse=0.9375, test_epoch=200** (0.9389->0.9382->0.9375)
               2. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=True, average_ratio=0.2, rmse=0.8720, test_epoch=795, rmse=0.9336, test_epoch=200** (0.9354 -> 0.9346 -> 0.9336)
               3. **node=300, epoch=800, local_epoch=5, optimizer=SGD, scheduler=cosine, shuffle=True, average_ratio=0.3, rmse=0.8684, test_epoch=788, rmse=0.9298, test_epoch=200** (0.9309 -> 0.9303 -> 0.9298)
            5. 不放回抽样, 每个样本不被选中的概率（299...270)/(300...271)=0.9，5次不被选中的概率0.9^5: , 每轮所以一共有（期望值）30 * p 个样本一次都不会被平均
               1. 当x为3时, 17.7
               2. 当x为6时, 9.83
               3. 当x为9时, 5.04
               4. 按照分析，应该从0->3->6->9，都会提升，其中0->3->6提升大
      
   10. 2/21/2022
       1. **Fine tune, 使用Adam**
          1. 效果一般, lr=0.001, 0.0001, 0.00001, with cosine, multistep
       2. **Check shuffle average最后效果**：
       3. **fix bug, first 100node, then 1user/node,  no local optimizer/local optimizer,** 
          1. **fix bug:**
             1. 改进流程
             2. 使用global scheduler
          2. **100node, with local optimizer:**
             1. node=100, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  average_decoder=True, rmse=0.8614, epoch=760
          3. **300node, with local optimizer:**
             1. node=300, fraction=0.1, global_epoch=800, local epoch=5,  加layernorm, local_optimizer=True, global_scheduler_lr=True,  average_decoder=True, rmse=0.8601, epoch=790
          4. **1user/node, without local optimizer:**
             1. 跳过
          5. **1user/node, with local optimizer**:
             1. 在下处优化
       4. **find dataset，分item attribute**
          1. 下处
       5. **New autoencoder （暂时跳过）**
          1. x(embedding) 1 
          2. x 2 
          3. x 3 
          4. x 4 
          5. x 5
       
   11. 2/24/2022
       1. 跑1M (1 user/node), 10M, 20M (100node)
          1. **改进command** (**整理， 削减参数**)
          2. **改进逻辑，流程 (参数传递，cpu/gpu)**
          3. **改进bash**
          3. **改进logger（ 存和取(test) for local model minimum）**
          
       2. 改进最后一层
          1. **改进逻辑 (计算更新的位置)**
          2. **计算空间占比** (100node decoder compress最后一层 => 60%)
          
       3. find dataset
          1. 要求:
             1. **不是Movie**
             2. **数据量至少几十万1m往上**
             3. **要有用户标签**
          
          2. Netflix Prize Dataset
          
             1. https://www.kaggle.com/netflix-inc/netflix-prize-data
             2. Movie => 略过
          
          3. Douban
          
             1. movie => 略过
          
          4. Douban Monti
          
             1. douban数据经过monti这个人处理, 分割成subset (we use the preprocessed subsets and splits provided by Monti et al)
             2. movie => 略过
             3. https://proceedings.neurips.cc/paper/2017/file/2eace51d8f796d04991c831a07059758-Paper.pdf
          
          5. Flixster Monti
          
             1. Flixster is a social movie site allowing users to share movie ratings, discover new movies and meet others with similar movie taste.
             2. 社交网络 => 略过
          
          6. Gowalla
          
             1. https://snap.stanford.edu/data/loc-gowalla.html
          
             2. Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010.
          
             3. 基于地理位置的用户兴趣推荐
          
                
          
          7. YahooMusic
          
             1. https://webscope.sandbox.yahoo.com/catalog.php?datatype=r
             2. musical artists
             3. 多种数据集:
                1. R1
                   1. Music
                   2. 数据量: 10 millions
                   3. User标签：无 （meaningless anonymous numbers）
                   4. item标签: 无
                2. R2
                   1. Music
                   2. 数据量: 717 millions
                   3. User标签：无 （meaningless anonymous numbers）
                   4. item标签: artist, album, genre
                3. R3
                   1. Music
                   2. 数据量: 0.35million
                   3. User标签：无 （meaningless anonymous numbers）
                   4. item标签:  无
          
          8. YahooMusic Monti
          
             1. douban数据经过monti这个人处理, 分割成subset (we use the preprocessed subsets and splits provided by Monti et al)
          
          9. Million Song Dataset
          
             1. 信息搜集整合平台
             2. 搜集了来自SecondHandSongs dataset 、Last.fm dataset 等7个知名并且权威的音乐社区的数据。当中除了各大音乐网站的原始数据外，MSD还对它们进行了必要的分析和提取。
             2. https://www.researchgate.net/publication/220723656_The_Million_Song_Dataset
             2. Music
             2. 数据量: 280 GB of data / 1, 000, 000 songs/files
             2. User标签：
             3. item标签:  
          
          10. Amazon-Book
          
              1. http://jmcauley.ucsd.edu/data/amazon/
              2. Book
              2. 数据量: 1-22 millions
              4. User标签：ratings, text, helpfulness votes
                 1. {
                      "reviewerID": "A2SUAM1J3GNN3B",
                      "asin": "0000013714",
                      "reviewerName": "J. McDonald",
                      "helpful": [2, 3],
                      "reviewText": "I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!",
                      "overall": 5.0,
                      "summary": "Heavenly Highway Hymns",
                      "unixReviewTime": 1252800000,
                      "reviewTime": "09 13, 2009"
                    }
              5. item标签:  descriptions, category information, price, brand, and image features
          
              
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          1. Book-Crossings （最不密集的数据集之一）
             1. http://www2.informatik.uni-freiburg.de/~cziegler/BX/
             2. 数据量: 1.1M
             3. 用户标签: 含有 location, age
             4. 用户数量: 0.28M
             5. item数量: 0.27M本书籍
          
          2. Jester (评分密度高, -10到10):
             1. https://eigentaste.berkeley.edu/dataset/
             2. Dataset 1:
                1. 数据量: 4.1M
                2. 标签: 无，匿名
                3. 用户数量: 0.07M
                4. item数量: 100
             3. Dataset 2:
                1. 数据量: 2.3M
                2. 标签: 无
                3. 用户数量: 0.08M左右
                4. item数量: 150
          
          3. Last.fm （具有用户的社交网络的信息的数据集）
             1. 2千名用户的社交网络、tagging和music artist listening信息。
             2. 数据量： 0.1M
             3. 标签: 无
             4. 用户数量: 2000
             5. item数量: 10左右
          
          4. EachMovie
             1. https://www.cs.cmu.edu/~lebanon/IR-lab/data.html
             2. 数据量: 2.8M
             3. 标签: Age, Gender
             4. 用户数量: 0.07M
             5. item数量: 1628 
          
          5. Steam Video Games （包括用户行为，有利于隐式反馈推荐算法的研究，同时研究用户对各游戏的喜好程度，因此用来做数据分析也不错）
             1. https://www.kaggle.com/tamber/steam-video-games/data
             2. 数据量: 0.2M
             3. 标签: 无
             4. 用户数量: 0.1M
             5. item数量: 5155games
          
          6. Netflix Prize Data Set
             1. https://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a
             2. 数据量: 100M
             3. 标签: 无
             4. 用户数量: 0.48M
             5. item数量: 17770
          
          7. Yahoo Dataset:
             1. https://webscope.sandbox.yahoo.com/catalog.php?datatype=r
          
          8. **Polyvore （私人衣服穿着搭配， personal fashion recommendation, 感觉是cv相关，需要识别图片）**
             1. https://github.com/xthan/polyvore-dataset
             2. paper: https://people.cs.clemson.edu/~jzwang/ustc16/mm2015/p129-hu.pdf
             3. 数据量: 0.02M
             4. 标签: 无用户标签
          
          9. Epinions
             1. https://paperswithcode.com/dataset/epinions
             2. Social Network
          
          10. Flixster
              1. https://paperswithcode.com/sota/collaborative-filtering-on-flixster
              2. Social Network
          
          11. Pinterest
              1. https://sites.google.com/site/xueatalphabeta/academic-projects
              2. 用户历史关注的图片的标签作为user特征；图片本身即image特征
          
          12. **Libimseti（dating）**
              1. http://konect.cc/networks/libimseti/
              2. Subset: https://www.kaggle.com/competitions/rice-stat-640-444/data
              3. Dating
              4. 数据量: 3M / 20M
              5. User label: Gender
              6. item label: profile id
          
          13. OkCupid
              1. https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles
              2. Dating
              3. 数据量: 0.06M
              4. 
          
          14. frappe
              1. https://github.com/hexiangnan/neural_factorization_machine/tree/master/data/frappe
              2. 用户使用app的数据
              3. 数据量: 0.1M
              4. 用户标签: 使用时候的country, city, weather等
          
          15. Amazon Beauty
              1. https://www.kaggle.com/datasets/skillsmuggler/amazon-ratings
              2. 对化妆品的打分
              3. 数据量: 142.8 million
              4. 用户标签: 无
          
          16. Amazon Games
              1. https://jmcauley.ucsd.edu/data/amazon/
              2. 用户标签: 无
          
          17. Amazon Product Data
              1. https://jmcauley.ucsd.edu/data/amazon/
              2. 用户标签: 无
          
          18. Wechat
              1. https://github.com/yaqingwang/WeFEND-AAAI20
              2. 检测fake news
              3. 数据量: 0.02M
              4. 用户标签: 无
              5. item标签: titile, news url, image url, report content, etc.
          
          19. DBbook2014
              1. 数据呢？the original data is limited to their authority.
              2. https://drive.google.com/file/d/1FIbaWzP6AWUNG2-8q6SKQ3b9yTiiLvGW/view
              3. 知识图谱数据集
          
          20. 介绍数据集链接: 
              1. https://www.jianshu.com/p/10daab68ad82
              2. https://www.cnblogs.com/shenxiaolin/p/8337913.html
              2. https://cseweb.ucsd.edu/~jmcauley/datasets.html
          
          21. 淘宝展示广告点击率预估数据集(classification)
          
              1. https://www.kaggle.com/datasets/pavansanagapati/ad-displayclick-data-on-taobaocom
              2. https://tianchi.aliyun.com/dataset/dataDetail?dataId=56
              3. 给用户展示广告，点击与否与后续22天的购物记录
              4. 数据量: 1.14M用户，26M数据
              2. 用户标签: gender, age, 消费档次，购物深度，是否大学生，城市层级
              3. item标签: category ID(商品种类), campaign ID(广告计划), brand(品牌), customer id(被投放的人)，价格等
          
          22. 淘宝APP中云主题产品的用户点击数据
          
              1. https://tianchi.aliyun.com/dataset/dataDetail?dataId=9716
              2. 数据量: 1.4M点击日志
              3. 用户标签: 只有用户购买商品与否
              4. item标签: 商品对应的主题，对应的最低层级/最高层级类目ID
          
          23. 移动端智能数据集
          
              1. https://tianchi.aliyun.com/dataset/dataDetail?dataId=109858
              2. 研究商品展示序列对点击的影响
              3. 数据量: 8M user, 50M item, 13M数据
              4. 用户标签: gender, age, 消费力评级, 曝光页码顺序等
          
          24. User Behavior Data from Fliggy Trip Platform for Recommendation（旅行记录）
          
              1. https://tianchi.aliyun.com/dataset/dataDetail?dataId=113649
              2. 基于过往behavior推荐数据
              3. 数据量: 5.5M user, 2.7M item, 220M数据
              2. 用户标签: gender, age, 消费力评级, 曝光页码顺序等
          
          25. Baby Goods Info Data
          
              1. https://tianchi.aliyun.com/dataset/dataDetail?dataId=45
              2. Predict children's ages based on their parents' purchase behavior, or predict what kind of goods a user would buy based on their children's info (age, gender etc.
              3. 预测孩子的岁数基于家长的purchase behavior
              4. 数据量: 9M孩子
              5. 用户标签: gender, 出生日期
              6. item标签: category ID, quantity, etc
          
          26. Goodreads Book Datasets
          
              1. https://www.kaggle.com/datasets/bahramjannesarr/goodreads-book-datasets-10m?select=user_rating_0_to_1000.csv
              2. 无用户标签
          
          27. Google Local Reviews:
          
              1. https://cseweb.ucsd.edu/classes/wi15/cse255-a/reports/wi15/Xinchi_Gu_Long_Jin.pdf
              2. https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local
              3. 个人对地点的打分
          
              
          
          数据详细描述:
          
          ​	淘宝展示广告点击率预估数据集(classification)
          
          1. https://www.kaggle.com/datasets/pavansanagapati/ad-displayclick-data-on-taobaocom
          2. https://tianchi.aliyun.com/dataset/dataDetail?dataId=56
          3. 给用户展示广告，点击与否与后续22天的购物记录
          4. 数据量: 1.14M用户，26M数据
          5. 用户标签: gender, age, 消费档次，购物深度，是否大学生，城市层级
          6. item标签: category ID(商品种类), campaign ID(广告计划), brand(品牌), customer id(被投放的人)，价格等
          
          **原始样本骨架raw_sample**
          
          1. 1. 1. (1) user_id：脱敏过的用户ID；
                   (2) adgroup_id：脱敏过的广告单元ID；
                   (3) time_stamp：时间戳；
                   (4) pid：资源位；
                   (5) noclk：为1代表没有点击；为0代表点击；
                   (6) clk：为0代表没有点击；为1代表点击；
          
                   我们用前面7天的做训练样本（20170506-20170512），用第8天的做测试样本（20170513）。
          
             2. **广告基本信息表ad_feature**
          
                1. (1) adgroup_id：脱敏过的广告ID；
                   (2) cate_id：脱敏过的商品类目ID；
                   (3) campaign_id：脱敏过的广告计划ID；
                   (4) customer_id:脱敏过的广告主ID；
                   (5) brand：脱敏过的品牌ID；
                   (6) price: 宝贝的价格
                   其中一个广告ID对应一个商品（宝贝），一个宝贝属于一个类目，一个宝贝属于一个品牌。
          
             3. **用户基本信息表user_profile**
          
                1. (1) userid：脱敏过的用户ID；
                   (2) cms_segid：微群ID；
                   (3) cms_group_id：cms_group_id；
                   (4) final_gender_code：性别 1:男,2:女；
                   (5) age_level：年龄层次；
                   (6) pvalue_level：消费档次，1:低档，2:中档，3:高档；
                   (7) shopping_level：购物深度，1:浅层用户,2:中度用户,3:深度用户
                   (8) occupation：是否大学生 ，1:是,0:否
                   (9) new_user_class_level：城市层级
          
          
----

Paper讨论部分:

1. Fedavg + autoencoder
2. Personalization (上传rate的部分 n / n')
1. **batchnorm，group norm(layernorm) 区别**
4. paper实验:
   1. 对比layernorm, groupnorm
      1. movielens
         1. ML100K (每个都分为不加Norm, batchnorm, groupnorm)
            1. joint
            2. 100nodes
         2. ML1M  (每个都分为不加Norm, batchnorm, groupnorm)
            1. joint
            2. 100nodes
         3. ML10M  (每个都分为不加Norm, batchnorm, groupnorm)
            1. joint
            2. 100nodes
         4. ML20M  (每个都分为不加Norm, batchnorm, groupnorm)
            1. joint
            2. 100nodes
      2. 新数据集
         1. 同上
   2. **以下实验都是layernorm----------**
   3. 测试Joint (**upperbound**)
      1. movielens, 
         1. ML100K
         2. ML1M
         3. ML10M
         4. ML20M
      2. 新数据集
   5. 测试fedavg
      1. 获取rmse/MAP
      2. 获取空间占比compress与不compress
      3. 获取在使用Global model minimum与local model minimum情况下准确度的不同 （需要验证）
      4. movielens
         1. explicit
            1. ML100K
               1. 100nodes
               3. 1user/node 
            2. ML1M
               1. 100nodes
               3. 1user/node
            3. ML10M
               1. 100nodes 
               1. 300nodes
            4. ML20M
               1. 100nodes
               1. 300nodes
            5. ML100K（only decoder）
               1. 100nodes （compress / no compress）
               3. 1user/node （compress / no compress）
            6. ML1M（only decoder）
               1. 100nodes（compress / no compress）
               3. 1user/node （compress / no compress）
            7. ML10M（only decoder）
               1. 100nodes（compress / no compress）
               2. 300nodes （compress / no compress）
            8. *ML20M （only decoder）*
               1. *100nodes（compress / no compress）*
               2. *300nodes （compress / no compress）*
         2. implicit
            1. explicit
               1. ML100K
                  1. 100nodes
                  2. 300nodes 
                  3. 1user/node 
               2. ML1M
                  1. 100nodes
                  2. 300nodes
                  3. 1user/node
               3. ML10M
                  1. 100nodes 
                  2. 300nodes
               4. ML20M
                  1. 100nodes
                  2. 300nodes
               5. ML100K（only decoder）
                  1. 100nodes （compress / no compress）
                  2. 300nodes （compress / no compress）
                  3. 1user/node （compress / no compress）
               6. ML1M（only decoder）
                  1. 100nodes（compress / no compress）
                  2. 300nodes（compress / no compress）
                  3. 1user/node （compress / no compress）
               7. ML10M（only decoder）
                  1. 100nodes（compress / no compress）
                  2. 300nodes （compress / no compress）
               8. ML20M （only decoder）
                  1. 100nodes（compress / no compress）
                  2. 300nodes （compress / no compress）
      5. 新数据集
         1. explicit
         2. implicit
   6. 总结:
      1. layernorm, batchnorm, no norm对比
      2. fedavg准确度曲线:
         1. movielens
            1. ML10K, fedsgd, joint, 100nodes, 300nodes, 1user/node (explicit和implicit)
            2. ML1M, fedsgd, joint, 100nodes / 300nodes / 1user/node (explicit和implicit)
            3. ML10M, fedsgd, joint, 100nodes / 300nodes (explicit和implicit)
            4. ML20M, fedsgd, joint, 100nodes / 300nodes (explicit和implicit)
         2. 新数据集
            1. 111
      3. compress效果对比
         1. movielens
            1. ML10K, 100nodes, 300nodes, 1user/node （compress与不compress）
            2. ML1M, 100nodes / 300nodes / 1user/node （compress与不compress）
            3. ML10M, 100nodes / 300nodes （compress与不compress）
            4. ML20M, 100nodes / 300nodes （compress与不compress）
         2. 新数据集
      4. global model minimum local model minimum (**discuss**)
5. Scheduler
   1. optimizer
      1. SGD
         1. ![image-20220220160018790](/Users/qile/Library/Application Support/typora-user-images/image-20220220160018790.png)
      2. Adam
         1. ![image-20220220160036074](/Users/qile/Library/Application Support/typora-user-images/image-20220220160036074.png)
      3. LBFGS
         1. 拟牛顿法
   2. Scheduler
      1. StepLR
         1. ![image-20220220160731155](/Users/qile/Library/Application Support/typora-user-images/image-20220220160731155.png)
      2. MultiStepLR
         1. ![image-20220220160741324](/Users/qile/Library/Application Support/typora-user-images/image-20220220160741324.png)
      3. ExponentialLR
         1. ![image-20220220160751226](/Users/qile/Library/Application Support/typora-user-images/image-20220220160751226.png)
      4. ConsineAnnealingLR
         1. ![image-20220220160801983](/Users/qile/Library/Application Support/typora-user-images/image-20220220160801983.png)
      5. ReduceLROnPlateau
         1. ![image-20220220184409125](/Users/qile/Library/Application Support/typora-user-images/image-20220220184409125.png)
      6. CyclicLR
         1. ![image-20220220184840843](/Users/qile/Library/Application Support/typora-user-images/image-20220220184840843.png)
         2. ![image-20220220184346923](/Users/qile/Library/Application Support/typora-user-images/image-20220220184346923.png)

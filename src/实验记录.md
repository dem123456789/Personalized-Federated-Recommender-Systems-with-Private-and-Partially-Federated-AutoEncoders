实验记录:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 1, RMSE ≈ 1(最后收敛速度慢)
1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1(最后收敛速度慢)

----

改动平均方法和scheduler后:

1. SGD, lr = 0.1, local_epoch = 5, scheduler = CosinAnnealingLR, epoch = 800, fraction = 0.1, user = 943, batch_size = 10, RMSE ≈ 1.025





-------

重构代码Pipeline, debug后

1. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=100, epoch=800, dataset=ML100K, rmse ≈ 0.956
2. all_federated，frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, num_nodes=10, epoch=800, dataset=ML100K, rmse ≈ 0.956

-------

Debug后

1. all federated, frac=1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=1, epoch=47, dataset=ML100K, rmse = 0.9177 (Single Node)
2. all federated, frac=1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=1, epoch=15, dataset=ML100K, rmse = 0.9260 (Single Node)

1. all federated, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=69, dataset=ML100K, rmse = 0.9447
1. all federated, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=500, dataset=ML100K, rmse = 1.3819
1. Decoder, frac=0.1, local_epoch=5, optimizer=SGD, lr=0.1, scheduler=CosineAnnealingLR, num_nodes=100, epoch=70, dataset=ML100K, rmse =1.0470
1. Decoder, frac=0.1, local_epoch=5, optimizer=Adam, lr=1e-3, scheduler=None, num_nodes=100, epoch=47, dataset=ML100K, rmse = 1.5456